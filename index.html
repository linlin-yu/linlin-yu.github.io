<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Linlin Yu | PhD @ UTD</title>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/panda5.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Linlin Yu</name>
              </p>
              <p>I am a Ph.D. student in the Computer Science Department at <a href="https://www.utdallas.edu/">The University of Texas at Dallas</a>, 
                working under the supervision of <a href="https://personal.utdallas.edu/~fxc190007/">Prof. Feng Chen</a>. I also work closely with 
                <a href="https://sites.google.com/site/louyifei/">Prof. Yifei Lou</a>.
                I received my B.E. degree 
                in Information Security from <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a> in 2019. 
              </p>
              <p>
                My research focuses on evidential uncertainty quantification and reasoning for complex structural data. 
                I aim to improve the reliability of uncertainty estimations by integrating domain-specific prior knowledge. 
                My work has applications in areas such as attributed graphs, hyperspectral imaging classification, bird's-eye view semantic segmentation, and generative models. 
              </p>
              <p>
                I am actively seeking research opportunities, including <strong>internships</strong> or <strong>full-time</strong> positions, in areas related to Deep Learning, Machine Learning, and Data Mining.  
                I'd love to get in contact.
              </p>

<!--               <font color="red"><strong>Seeking for full-time position in America starting from 2022</strong></font>  -->
              <p style="text-align:center">
                <a href="mailto:linlin.yu@utdallas.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=3pu55HoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/linlin-yu-723884249/">LinkedIn</a> &nbsp/&nbsp
                <!-- <a href="https://github.com/linlin-yu">Github</a>  -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/person_1.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/person_1.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Quote</heading>
              <p>
                "<i> The fear of the LORD is the beginning of wisdom </i> " - Psalms 111:10
              </p>
            </td>
          </tr>
        </tbody></table> -->
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p></p>
              <li> 05/22025: I started my summer research internship at <a href='https://www.nec-labs.com/'>NEC Laboratories America</a></li>
                <li> 03/2025: Our <a href='https://hal.science/hal-04973361v1/file/Survey_on_Uncertainty_Estimation_in_Large_Language_Models__Sources__Methods__Applications__Challenges%20%2820%29.pdf'>survey</a> paper on uncertainty quantification in LLM is available online </li>
                <li>01/2025: One paper got accepted by <strong>ICLR 2025</strong></li>
                <li>01/2025: One paper got accepted by <strong>AISTATS 2025</strong></li>
                <li>10/2024: One paper got accepted by <strong>Frontiers in BigData 2024</strong></li>
                <li>10/2024: I will serve as the reviewer of <strong>ICLR,AISTATS,NeurIPS 2025</strong></li>
                <li>09/2024: One paper got accepted by <strong>EMNLP 2024</strong></li>
                <li>07/2024: I will serve as the reviewer of <strong>KDD 2025, BigData 2024</strong></li>
                <li>05/2024: I will give a talk on "Evidential Deep Learning for Uncertainty Quantification" in Tianjin University</li>
                <li>05/2024: We release the code and dataset for the first benchmark on uncertainty-aware bird's eye view segmentation</li>
                <li>05/2024: I will serve as the reviewer of <strong>NeurIPS 2024</strong></li>
                <li>03/2024: One paper got accepted by <strong>NAACL 2024 Finidings</strong></li>
                <li>01/2024: One paper got accepted by <strong>ICLR 2024 </strong></li>
                <li>09/2023: One paper got accepted by <strong>NeurIPS 2024 </strong></li>
                <li>06/2023: One paper got accepted by 2nd <strong>KDD workshop</strong> on Uncertainty Reasoning and Quantification in Decision Making</li>
              </p>
              </p>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
              <p>
                <li>Reseracher</b> at NEC Laboratories America (Princeton, NJ) since 2022 summer</li>
                <li>Reserach intern</b> at NEC Laboratories America (Princeton, NJ) during 2021 summer</li>
                <li>Reserach intern</b> at Alibaba Damo Academy (Seattle, WA) during 2019 summer</li>
              </p>
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in machine learning and data mining, especially in Uncertainty Estimation, Trustworthy Large Language Models, and Graph Neural Networks.
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="UQ_Survey_stop()" onmouseover="Preprint2024_start()", bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='Preprint2024_image'>
                  <img src='images/UQ_Survey_after.png' width="160"></div>
                <img src='images/UQ_Survey_before.png' width="160">
              </div>
              <script type="text/javascript">
                function UQ_Survey_start() {
                  document.getElementById('Preprint2024_image').style.opacity = "1";
                }

                function UQ_Survey_stop() {
                  document.getElementById('Preprint2024_image').style.opacity = "0";
                }
                UQ_Survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hal.science/hal-04973361v1/file/Survey_on_Uncertainty_Estimation_in_Large_Language_Models__Sources__Methods__Applications__Challenges%20%2820%29.pdf">
                <papertitle>Survey of Uncertainty Estimation in LLMs - Sources, Methods, Applications, and Challenge</papertitle>
              </a>
              <br>
              Jianfeng He *<strong>Linlin Yu *</strong>, Changbin Li *, et al.
              <br>
              <em>Preprint</em>, 2025
              <br>
              <a href="https://hal.science/hal-04973361v1/file/Survey_on_Uncertainty_Estimation_in_Large_Language_Models__Sources__Methods__Applications__Challenges%20%2820%29.pdf">paper</a> 
              <p></p>
              <p> This survey provides a comprehensive overview of uncertainty estimation for LLMs from the perspective of uncertainty sources, serving as a foundational resource for researchers entering the field. We begin by reviewing essential background on LLMs, followed by a detailed clarification of uncertainty sources relevant to them. We then introduce various uncertainty estimation methods, including both commonly used and LLM-specific approaches. Metrics for evaluating uncertainty are discussed, along with key application areas. Finally, we highlight major challenges and outline future research directions aimed at improving the trustworthiness and reliability of LLMs.</p>
            </td>
          </tr>

          <tr onmouseout="AISTATS2025_stop()" onmouseover="ICLR2025_start()", bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='AISTATS2025_image'>
                  <img src='images/AISTATS2025_after.png' width="160"></div>
                <img src='images/AISTATS2025_before.png' width="160">
              </div>
              <script type="text/javascript">
                function AISTATS2025_start() {
                  document.getElementById('AISTATS2025_image').style.opacity = "1";
                }

                function AISTATS2025_stop() {
                  document.getElementById('AISTATS2025_image').style.opacity = "0";
                }
                AISTATS2025_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2405.20986">
                <papertitle>Evidential Uncertainty Probes for Graph Neural Networks</papertitle>
              </a>
              <br>
              <strong>Linlin Yu</strong>, Kangshuo Li, Pritom Kumar Saha, Yifei Lou, Feng Chen
              <br>
              <em>AISTATS</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2405.20986">paper</a> /
              <a href="https://github.com/linlin-yu/Evidential-Probing-GNN.git">code</a>/
              <a href="https://virtual.aistats.org/media/PosterPDFs/AISTATS%202025/9463.png?t=1745369726.432262">poster</a>
              <p></p>
              <p> We propose a plug-and-play framework for uncertainty quantification in Graph Neural Networkss that works with pre-trained models without the need for retraining. Our Evidential Probing Network uses a lightweight Multi-Layer-Perceptron head to extract evidence from learned representations, allowing efficient integration with various GNN architectures.  We further introduce evidence-based regularization techniques, referred to as EPN-reg, to enhance the estimation of epistemic uncertainty with theoretical justifications. Extensive experiments demonstrate that the proposed EPN-reg achieves state-of-the-art performance in accurate and efficient uncertainty quantification, making it suitable for real-world deployment.</p>
            </td>
          </tr>

          <tr onmouseout="ICLR2025_stop()" onmouseover="EMNLP2024_start()", bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICLR2025_image'>
                  <img src='images/ICLR2025_after.png' width="160"></div>
                <img src='images/ICLR2025_before.png' width="160">
              </div>
              <script type="text/javascript">
                function ICLR2025_start() {
                  document.getElementById('ICLR2025_image').style.opacity = "1";
                }

                function ICLR2025_stop() {
                  document.getElementById('ICLR2025_image').style.opacity = "0";
                }
                ICLR2025_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=k3y0oyK7sn">
                <papertitle>Uncertainty Quantification for Bird's Eye View Semantic Segmentation: Methods and Benchmarks</papertitle>
              </a>
              <br>
              <strong>Linlin Yu *</strong>, Bowen Yang *, Tianhao Wang, Kangshuo Li, Feng Chen
              <br>
              <em>ICLR</em>, 2025
              <br>
              <a href="https://openreview.net/pdf?id=k3y0oyK7sn">paper</a> /
              <a href="https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization">code</a> /
              <a href="https://iclr.cc/media/PosterPDFs/ICLR%202025/28599.png?t=1745272360.0748715">poster</a> 
              <p></p>
              <p> This study introduces a comprehensive benchmark for predictive uncertainty quantification in BEV segmentation, evaluating multiple uncertainty quantification methods across three popular datasets with three representative network architectures. we propose a novel loss function, Uncertainty-FocalCross-Entropy (UFCE), specifically designed for highly imbalanced data, along with a simple uncertainty-scaling regularization term that improves both uncertainty quantification and model calibration for BEV segmentation.</p>
            </td>
          </tr>

          <tr onmouseout="EMNLP2024_stop()" onmouseover="FRONTIERS2024_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='EMNLP2024_image'>
                  <img src='images/EMNLP2024_after.png' width="160"></div>
                <img src='images/EMNLP2024_before.png' width="160">
              </div>
              <script type="text/javascript">
                function EMNLP2024_start() {
                  document.getElementById('EMNLP2024_image').style.opacity = "1";
                }

                function EMNLP2024_stop() {
                  document.getElementById('EMNLP2024_image').style.opacity = "0";
                }
                EMNLP2024_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/2024.emnlp-main.923.pdf">
                <papertitle>Can We Trust the Performance Evaluation of Uncertainty Estimation
                  Methods in Text Summarization?</papertitle>
              </a>
              <br>
              Jianfeng He, Runing Yang, <strong>Linlin Yu</strong>, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu
              <br>
              <em>EMNLP</em>, 2024
              <br>
              <a href="https://aclanthology.org/2024.emnlp-main.923.pdf">paper</a> /
              <a href="https://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization">code</a> 
              <p></p>
              <p> In this paper, we introduce a comprehensive uncertainty estimation on text summarization benchmark incorporating 31 NLG metrics across four dimensionse. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of uncertainty-aware text summarization techniques</p>
            </td>
          </tr>
          
          <tr onmouseout="FRONTIERS2024_stop()" onmouseover="NAACL2024_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='FRONTIERS2024_image'>
                  <img src='images/FRONTIERS2024_after.png' width="160"></div>
                <img src='images/FRONTIERS2024_before.png' width="160">
              </div>
              <script type="text/javascript">
                function FRONTIERS2024_start() {
                  document.getElementById('FRONTIERS2024_image').style.opacity = "1";
                }

                function FRONTIERS2024_stop() {
                  document.getElementById('FRONTIERS2024_image').style.opacity = "0";
                }
                FRONTIERS2024_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1431346/full">
              <papertitle>Camera-view Supervision for Bird's-Eye-View Semantic Segmentation</papertitle>
              </a>
              <br>
              Bowen Yang, <strong>Linlin Yu</strong>, Feng Chen
              <br>
              <em>Frontiers in BigData</em>, 2024
              <br>
              <a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1431346/full">paper</a>/
              <a href="https://github.com/bluffish/sucam">code</a> 
              <p></p>
              <p> We propose a method of supervising feature extraction with camera-view depth and segmentation information, which improves the quality of feature extraction and projection in the Bird's eye view semantic segmentation pipeline. Our model, evaluated on the nuScenes dataset, shows a 3.8% improvement in Intersection-over-Union (IoU) for vehicle segmentation and a 30-fold reduction in depth error compared to baselines, while maintaining competitive inference times of 32 FPS. This method offers more accurate and reliable BEVSS for real-time autonomous driving systems.  </p>
            </td>
          </tr>

    
          <tr onmouseout="NAACL2024_stop()" onmouseover="ICLR2024_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='NAACL2024_image'>
                  <img src='images/NAACL2024_after.png' width="160"></div>
                <img src='images/NAACL2024_before.png' width="160">
              </div>
              <script type="text/javascript">
                function NAACL2024_start() {
                  document.getElementById('NAACL2024_image').style.opacity = "1";
                }

                function NAACL2024_stop() {
                  document.getElementById('NAACL2024_image').style.opacity = "0";
                }
                NAACL2024_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/2024.findings-naacl.180.pdf">
                <papertitle>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</papertitle>
              </a>
              <br>
              Jianfeng He, <strong>Linlin Yu</strong>,Shuo Lei, Chang-Tien Lu, Feng Chen
              <br>
              <em>NAACL Findings</em>, 2024 
              <br>
              <a href="https://aclanthology.org/2024.findings-naacl.180.pdf">paper</a> /
              <a href="https://github.com/he159ok/UncSeqLabeling_SLPN">code</a> 
              <p></p>
              <p> we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on three datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.</p>
            </td>
          </tr>


          <tr onmouseout="ICLR2024_stop()" onmouseover="NIPS2024_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICLR2024_image'>
                  <img src='images/ICLR2024_after.png' width="160"></div>
                <img src='images/ICLR2024_after.png' width="160">
              </div>
              <script type="text/javascript">
                function ICLR2024_start() {
                  document.getElementById('ICLR2024_image').style.opacity = "1";
                }

                function ICLR2024_stop() {
                  document.getElementById('ICLR2024_image').style.opacity = "0";
                }
                ICLR2024_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=8dN7gApKm3">
                <papertitle>Uncertainty-aware Graph-based Hyperspectral Image Classification</papertitle>
              </a>
              <br>
              <strong>Linlin Yu</strong>, Yifei Lou, Feng Chen
              <br>
              <em>ICLR</em>, 2024 
              <!-- <em>NeurIPS</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>  -->
              <br>
              <a href="https://openreview.net/pdf?id=8dN7gApKm3">paper</a> /
              <a href="https://github.com/linlin-yu/uncertainty-aware-HSIC.git">code</a> /
              <a href="https://iclr.cc/media/PosterPDFs/ICLR%202024/19322.png?t=1714868237.0518255">poster</a> 
              <p></p>
              <p>In this paper, we adapt two advanced uncertainty quantification models, evidential GCNs (EGCN) and graph posterior networks (GPN), designed for node classifications in graphs, into the realm of Hyper Spectral Imaging Classification. We first reveal theoretically that a popular uncertainty cross-entropy loss function is insufficient to produce good epistemic uncertainty when learning EGCNs. To mitigate the limitations, we propose two regularization terms based on the inherent physical characteristics of hyperspectral data.</p>
            </td>
          </tr> 

          <tr onmouseout="NeurIPS2024_stop()" onmouseover="NeurIPS2024_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='NeurIPS2024_image'>
                  <img src='images/NeurIPS2024_after.png' width="160"></div>
                <img src='images/NeurIPS2024_before.png' width="160">
              </div>
              <script type="text/javascript">
                function NeurIPS2024_start() {
                  document.getElementById('NeurIPS2024_image').style.opacity = "1";
                }

                function NeurIPS2024_stop() {
                  document.getElementById('NeurIPS2024_image').style.opacity = "0";
                }
                NeurIPS2024_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/ad84864002a72c344c2227d7eb8842b1-Paper-Conference.pdf">
                <papertitle>Improvements on Uncertainty Quantification for Node Classification via Distance Based Regularization</papertitle>
              </a>
              <br>
              Russell Hart,
              <strong>Linlin Yu</strong>, Yifei Lou, Feng Chen
              <br>
              <em>NeurIPS</em>, 2024 
              <!-- <em>NeurIPS</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>  -->
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/ad84864002a72c344c2227d7eb8842b1-Paper-Conference.pdf">paper</a> /
              <a href="https://github.com/neoques/Graph-Posterior-Network">code</a> /
              <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202023/71909.png?t=1702266175.3741667">poster</a> 
              <p></p>
              <p>We theoretically analyze the limitations of Graph Posterior Network at OOD detection when minimizing uncertainty cross-entropy loss, and we propose a
                distance-based regularization that considers the prior knowledge that OOD-specific features are useful
                for learning representational space mappings.</p>
            </td>
          </tr> 

        </tbody></table>



  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/NIPS_logo.png" width="180"></td> -->
            <td width="75%" valign="center">
              <b>Conference Reviewer</b>
              <li>2025: NeurIPS, ICML, ICLR, KDD, AISTATS</li>
              <li>2024: KDD, NeurIPS, BigData, ICML</li>
              <li>2023: ICML </li>
            </td>
          </tr>
          <tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody><tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info/">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
